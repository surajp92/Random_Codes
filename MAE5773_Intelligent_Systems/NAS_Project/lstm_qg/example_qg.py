# -*- coding: utf-8 -*-
"""Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EzTMRhEofQY_VjsbNJGL1kCKVSy_24LG

## Load toy data and create evaluation function
"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn import datasets

seed(1)
from tensorflow import set_random_seed
set_random_seed(2)
import pandas as pd
import time as tm
from keras.regularizers import l2

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

import matplotlib.pyplot as plt

#%%
def create_training_data_lstm(training_set, m, n, lookback):
    ytrain = [training_set[i+1] for i in range(lookback-1,m-1)]
    ytrain = np.array(ytrain)
    xtrain = np.zeros((m-lookback,lookback,n))
    for i in range(m-lookback):
        a = training_set[i]
        for j in range(1,lookback):
            a = np.vstack((a,training_set[i+j]))
        xtrain[i] = a

    return xtrain, ytrain

#%%
lookback = 5
problem = "ROM"
slopenet = "LSTM"
legs = lookback

#%% read data for training
dataset_train = pd.read_csv('./a10.csv', sep=",",skiprows=0,header = None, nrows=900)
training_set = dataset_train.iloc[:,1:].values
m,n = training_set.shape

#scaling
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range=(-1,1))
training_set_scaled = sc.fit_transform(training_set)
training_set = training_set_scaled

X, y = create_training_data_lstm(training_set, m, n, lookback)

#%%
def evaluation_function(y_true, y_pred):
    true_mean = np.mean(y_true, axis=0)
    pred_mean = np.mean(y_pred, axis=0)
    diff = np.abs(true_mean - pred_mean)
    return np.mean(diff) 

def evaluate_model(learner, X, y, num_folds):
    mae = 0
    for train_ind, val_ind in KFold(n_splits=num_folds).split(X, y):
        learner.fit(X[train_ind, :], y[train_ind], epochs=10, batch_size=64)
        mae += evaluation_function(learner.predict(X[val_ind, :]), y[val_ind])

    print("MAE:", mae/num_folds)

def lstm_model(n_layers=2,n_cells=40,act_func=2,initializer=1,optimizer=1):
    lookback = 5
    n = 10
    
    act_func_dict = {1:'tanh',2:'relu',}
    initializer_dict = {1:'uniform',2:'glorot_normal',3:'random_normal'}
    optimizer_dict = {1:'adam',2:'rmsprop',3:'SGD'}
    
    model = Sequential()
    
    for k in range(n_layers-1):
        model.add(LSTM(n_cells, input_shape=(lookback, n), return_sequences=True, activation=act_func_dict[act_func], kernel_initializer=initializer_dict[initializer]))
        
    model.add(LSTM(n_cells, input_shape=(lookback, n), activation=act_func_dict[act_func], kernel_initializer=initializer_dict[initializer]))
    model.add(Dense(n,activation='linear'))
    
    model.compile(loss='mean_squared_error', optimizer=optimizer_dict[optimizer])
#    model.summary()
    
    return model

#learner = lstm_model(n_layers=2,n_cells=40) 
#evaluate_model(learner, X, y, 3)

#%%
from param import ContinuousParam, CategoricalParam, ConstantParam
from genetic_hyperopt import GeneticHyperopt

optimizer = GeneticHyperopt(lstm_model, X, y, mean_squared_error, maximize=False)

n_layers_param = ContinuousParam("n_layers", 4, 2, min_limit=2, max_limit=6, is_int=True)
n_cells_param = ContinuousParam("n_cells", 40, 10, min_limit=20, max_limit=60, is_int=True)
act_func_param = ContinuousParam("act_func", 1, 1, min_limit=1, max_limit=2, is_int=True)
initializer_param = ContinuousParam("initializer", 2, 1, min_limit=1, max_limit=3, is_int=True)
optimizer_param = ContinuousParam("optimizer", 2, 1, min_limit=1, max_limit=3, is_int=True)

optimizer.add_param(n_layers_param)
optimizer.add_param(n_cells_param)
optimizer.add_param(act_func_param)
optimizer.add_param(initializer_param)
optimizer.add_param(optimizer_param)

best_params, best_score, plotting_stats, best_param_dict = optimizer.evolve()

#%%
fig, ax = plt.subplots(1, 1, figsize=(6,5))

ax.plot(plotting_stats[:,0], plotting_stats[:,2],'ro-',ms=8)
ax.set_xlabel('$N$')
ax.set_ylabel('min($f$)')

fig.tight_layout()
plt.show()

np.savez('results_qg.npz',plotting_stats=plotting_stats,best_param_dict=best_param_dict)
