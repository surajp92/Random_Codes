{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Project-2_final.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"yktYWvVupcfq"},"source":["# Suraj Pawar \n","## A20163517\n"]},{"cell_type":"markdown","metadata":{"id":"koUIAAj9BM0C"},"source":["# Project-2: Locality Sensitive Hashing"]},{"cell_type":"code","metadata":{"id":"0IXykn4rBM0C"},"source":["import random\n","import os\n","import numpy as np\n","import collections\n","\n","from random import randrange\n","from shutil import copyfile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGxpwUw1gFGg","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1601680119189,"user_tz":300,"elapsed":217,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"61aa71e1-f76c-4774-cdba-f1c0f8f1e685"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sGd1gSnCBM0F"},"source":["## Execute the follwing two cells to generate your data sample"]},{"cell_type":"code","metadata":{"id":"af6b41pqBM0G"},"source":["MY_ID = '517'\n","# MY_ID should be string\n","# Example MY_ID = '819'\n","\n","task_dict = {0:'taska',1:'taskb',2:'taskc',3:'taskd',4:'taske'}\n","\n","n_1 = 0\n","n_2 = 0\n","n_3 = 0\n","\n","try:\n","    n_1 = int(MY_ID[-1]) % 5\n","    n_2 = int(MY_ID[-2]) % 5\n","    n_3 = int(MY_ID[-3]) % 5\n","    \n","    while n_1 == n_2 or n_1 == n_3:\n","        n_1 = (n_1 + 1) % 5\n","    \n","    while n_1 == n_2 or n_2 == n_3:\n","        n_2 = (n_2 + 1) % 5\n","\n","except Exception as e:\n","    print('Please enter a valid ID...')\n","\n","id_dict = {0:n_1,1:n_2,2:n_3}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxAyIP7VBM0K"},"source":["data_path = '/content/drive/My Drive/Coursework/CS 5683_Big_Data/Project-2/corpus-20090418' \n","# Edit this path if the data directory is not in the current directory\n","\n","try:\n","    os.makedirs('Data_Sample')\n","    os.makedirs('Original_Sample')\n","except Exception as e:\n","    pass\n","\n","for i in range(3):\n","    task = task_dict[id_dict[i]]\n","    \n","    for file in os.listdir(data_path):\n","        if task in file:\n","            if 'orig' in file:\n","                copyfile(data_path + '/' + file, 'Original_Sample/' + file)\n","            else:\n","                copyfile(data_path + '/' + file, 'Data_Sample/' + file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OiD9y848BM0N"},"source":["Your dataset for this project will be in <b>Data_Sample</b>\n","\n","\n","Your query choices will be in <b>Original_Sample</b> directory\n","\n","\n","You have to use any one original Wikipedia article from <b>Original_Sample</b> for <b>Fact Check</b> steps"]},{"cell_type":"markdown","metadata":{"id":"0sMikdXSBM0Q"},"source":["### STEP - 1: Shingling (20 points)"]},{"cell_type":"code","metadata":{"id":"3C5M1DhhBM0R"},"source":["# Type your code here... \n","# Create necessary number of cells below this cell\n","k1 = 3\n","k2 = 4\n","k3 = 5\n","\n","# NOTE: No complex text processing is required\n","# convert just upper case characters to lower case\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51n5YpGUPsGI"},"source":["dir_path = 'Original_Sample'\n","\n","doc_lines = []\n","counter = 0\n","for filename in os.listdir(dir_path):\n","  counter = counter + 1\n","  file = open(dir_path + '/' + filename, 'r',encoding='utf-8',errors='ignore')\n","  lines = file.read()\n","  doc_lines.append(lines)\n","\n","dir_path = 'Data_Sample'\n","for filename in os.listdir(dir_path):\n","  counter = counter + 1\n","  file = open(dir_path + '/' + filename, 'r',encoding='utf-8',errors='ignore')\n","  lines = file.read()\n","  doc_lines.append(lines)\n","\n","n_docs = len(doc_lines)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vCUPJBNeSyHx","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1601679591613,"user_tz":300,"elapsed":358,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"ccf7637b-4f22-4dbf-f25c-04ed5cf8ab1d"},"source":["shingles_dict = {}\n","for k in [k1,k2,k3]:\n","  shingles = []\n","  for doc_line in doc_lines:\n","    l = doc_line.lower().strip().split()\n","    for i in range(len(l)-k+1):\n","      k_sh = tuple(l[i:i+k])\n","      shingles.append(k_sh)\n","\n","  shingles_set = set(shingles)\n","  shingles_set_list = list(shingles_set)\n","  shingles_set_list = [' '.join(item) for item in shingles_set_list]\n","  shingles_dict[k] = shingles_set_list\n","\n","  print(f'k = {k} : {len(shingles_dict[k])}')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["k = 3 : 8086\n","k = 4 : 8848\n","k = 5 : 9220\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8pQvFKm8BM0X"},"source":["Report results (number of unique k-shingles) for k={3,4,5} below:\n","1. k=3: 8977\n","2. k=4: 9793\n","3. k=5: 10169"]},{"cell_type":"markdown","metadata":{"id":"u34btTr2BM0d"},"source":["### STEP - 2: Min-Hashing (40 points)"]},{"cell_type":"code","metadata":{"id":"TKDjMhXHBM0e"},"source":["# ***************************************************NEW***********************************************************\n","# Generate Hash functions - \n","    # We use (ax + b) mod N formula to permute shingle index\n","    # where a,b are random numbers, N total index size, and x is the index\n","# We need to do L permutations - In other words we need to have L permutations (lists) of new indexes\n","# Following function takes total index size N and L as arguments\n","    # And returns L new lists of size N\n","    \n","def get_hash_functions(N,L):\n","    hash_functions = []\n","    \n","    for itr in range(L):\n","        a=randrange(1,400)\n","        b=randrange(1,400)\n","        \n","        new_hash_function = []\n","        for i in range(N):\n","            new_hash_function.append((a * i + b) % N)\n","        \n","        hash_functions.append(new_hash_function)\n","    return hash_functions\n","        \n","# test\n","# hash_functions = get_hash_functions(N,50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xHH0SG-txcR"},"source":["n_docs_data = n_docs - 3\n","\n","n = 0\n","file_name_dict = {}\n","\n","dir_path = 'Data_Sample'\n","file_shingles_dict = {}\n","\n","for file in os.listdir(dir_path):\n","  file_name_dict[n] = file\n","  file = open(dir_path + '/' + file, 'r',encoding='utf-8',errors='ignore')\n","  \n","  doc_shingles = []\n","\n","  data = file.read()\n","  l = data.lower().strip().split()\n","  \n","  for i in range(len(l)-k+1):\n","    k_sh = tuple(l[i:i+k])\n","    doc_shingles.append(k_sh)\n","  doc_shingles = [' '.join(item) for item in doc_shingles]\n","  file_shingles_dict[n] = doc_shingles\n","  n = n + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BK0O6rwBM0i"},"source":["# Type your code here to generate all L hash functions\n","# Generate hash functions only for shingle index created for k=5\n","k = k3\n","shingles_k5 = shingles_dict[5]\n","N = len(shingles_k5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYPadBKGjeho","colab":{"base_uri":"https://localhost:8080/","height":625},"executionInfo":{"status":"ok","timestamp":1601605195039,"user_tz":300,"elapsed":29484,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"3a0d58e4-c66d-4dff-caf4-0a63ee913a5d"},"source":["for L in [50,100,200,500,1000]:\n","#for L in [50]:\n","  hash_functions = get_hash_functions(N,L)\n","  M = np.full((L,n_docs_data), np.inf)\n","\n","  for key,doc_shingles in file_shingles_dict.items():\n","    for j in range(N):\n","      if shingles_k5[j] in doc_shingles:\n","        for i in range(L):\n","          if hash_functions[i][j] < M[i,key]:\n","            M[i,key] = hash_functions[i][j]\n","\n","  print(M)\n","  \n","  # print(f'The shape of the signature matrix is [{M.shape[0]}, {M.shape[1]}]')\n","\n","  # Type your code here to do the fact check \n","  #      with any one query document in the 'Original_Sample' directory\n","\n","  original = 'Original_Sample/orig_taska.txt'\n","\n","  # STEP-1: Generate 5-shingles \n","      # (if any shingles are no t present in your shingle index, simply ignore them)\n","\n","  S = np.zeros(N)\n","\n","  file = open(original, 'r',encoding='utf-8',errors='ignore')\n","  data = file.read()\n","\n","  doc_shingles = []\n","  l = data.lower().strip().split()\n","  for i in range(len(l)-k+1):\n","    k_sh = tuple(l[i:i+k])\n","    doc_shingles.append(k_sh)\n","\n","  doc_shingles = [' '.join(item) for item in doc_shingles]\n","      \n","  # STEP-2: Generate signature vector from L hash functions\n","  Mo = np.full((L,1), np.inf) \n","  for j in range(N):\n","    if shingles_k5[j] in doc_shingles:\n","      for i in range(L):\n","        if hash_functions[i][j] < Mo[i,0]:\n","          Mo[i,0] = hash_functions[i][j]  \n","\n","\n","  # STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.\n","      # and all other documents \n","  t = 0.2\n","  similar_docs = {}\n","  for i in range(n_docs_data):\n","    same = M[:,i] == Mo[:,0]\n","    count = sum(same)/L\n","    if count >= t:\n","      similar_docs[file_name_dict[i]] = count*100\n","      # print(i, ' ', file_name_dict[i], ' ', count*100)\n","  \n","  similar_docs_sorted = {k: v for k, v in sorted(similar_docs.items(), key=lambda item: item[1])}\n","\n","  print (\"{:<5} {:<25} {:<10} \".format('L', 'Name of the document', 'Jaccard similarity'))\n","  for key,value in similar_docs_sorted.items():\n","    print (\"{:<5} {:<25} {:<10} \".format(L, key, round(value,2)))\n","    # print(L, ' ', key, ' ', round(value,2))\n","  \n","  print('---------------------------------------------------')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["L     Name of the document      Jaccard similarity \n","50    g0pD_taska.txt            20.0       \n","50    g2pE_taska.txt            26.0       \n","50    g2pC_taska.txt            32.0       \n","50    g3pC_taska.txt            42.0       \n","50    g4pC_taska.txt            84.0       \n","50    g0pE_taska.txt            88.0       \n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","100   g0pD_taska.txt            20.0       \n","100   g2pC_taska.txt            26.0       \n","100   g3pC_taska.txt            30.0       \n","100   g4pC_taska.txt            85.0       \n","100   g0pE_taska.txt            89.0       \n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","200   g2pC_taska.txt            26.5       \n","200   g0pD_taska.txt            28.5       \n","200   g3pC_taska.txt            38.5       \n","200   g4pC_taska.txt            87.0       \n","200   g0pE_taska.txt            92.0       \n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","500   g0pD_taska.txt            24.0       \n","500   g2pC_taska.txt            33.4       \n","500   g3pC_taska.txt            36.6       \n","500   g4pC_taska.txt            84.4       \n","500   g0pE_taska.txt            89.6       \n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","1000  g0pD_taska.txt            21.0       \n","1000  g3pC_taska.txt            33.7       \n","1000  g2pC_taska.txt            34.1       \n","1000  g4pC_taska.txt            84.2       \n","1000  g0pE_taska.txt            87.8       \n","---------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Huw0RAFyBM0v"},"source":["***************************************************NEW***********************************************************\n","For each L = {50,100,200,500,1000}, report all documents (file_names) below that have Jaccard similarity > t=0.2\n","Sort the documents in decreasing order of the Jaccard similarity\n","\n","Below are the results I got for one of the run. Results might be little different based on the random number used in hash functions. \n","\n","---------------------------------------------------\n","\n","L     Name of the document      Jaccard similarity \n","\n","50    g3pC_taska.txt            22.0       \n","50    g0pD_taska.txt            26.0       \n","50    g2pC_taska.txt            30.0       \n","50    g4pC_taska.txt            88.0       \n","50    g0pE_taska.txt            92.0      \n","\n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","\n","100   g0pD_taska.txt            28.0       \n","100   g3pC_taska.txt            33.0       \n","100   g2pC_taska.txt            35.0       \n","100   g4pC_taska.txt            90.0       \n","100   g0pE_taska.txt            92.0    \n","\n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","\n","200   g3pC_taska.txt            28.5       \n","200   g0pD_taska.txt            29.0       \n","200   g2pC_taska.txt            30.5       \n","200   g4pC_taska.txt            85.0       \n","200   g0pE_taska.txt            89.0     \n","\n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","\n","500   g0pD_taska.txt            24.4       \n","500   g3pC_taska.txt            32.0       \n","500   g2pC_taska.txt            37.6       \n","500   g4pC_taska.txt            84.4       \n","500   g0pE_taska.txt            88.6   \n","\n","---------------------------------------------------\n","L     Name of the document      Jaccard similarity \n","\n","1000  g0pD_taska.txt            24.2       \n","1000  g3pC_taska.txt            32.7       \n","1000  g2pC_taska.txt            35.2       \n","1000  g4pC_taska.txt            83.9       \n","1000  g0pE_taska.txt            89.3       \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z8O-ieX6BM0y"},"source":["### STEP - 3: LSH (30 points)"]},{"cell_type":"code","metadata":{"id":"GxHhyaTEBM0z"},"source":["# Type your code here to hash signature matrix into B buckets\n","# Use the technique to split the signature matrix into b bands of r rows\n","# Convert only the signature matrix generated with L=1000\n","\n","b = 50\n","r = 20\n","B = 199\n","t = 0.2\n","\n","len_sim_docs = len(similar_docs_sorted)\n","\n","ar = random.sample(range(100), r)\n","# print(ar)\n","\n","ar = np.array(ar)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L8lLxaWbBM05"},"source":["# Type your code here to do generate candidate documents\n","# Follow all steps from STEP - 2 fact check (except the Jaccard similarity part)\n","\n","# STEP - 1: Split your original document signature vector into b bands of r rows\n","\n","bucket = collections.defaultdict(set)\n","\n","Mb = np.reshape(M,[b,r,n_docs_data]) \n","\n","# STEP - 2: Hash using the same hash functions created for \n","    # signature matrix hashing (in the previous cell)\n","\n","for i in range(b):\n","  dict_band = collections.defaultdict(list)\n","  \n","  for n in range(n_docs_data):\n","    x = Mb[i,:,n]\n","    h = (np.sum(ar*x)) % B\n","    dict_band[h].append(file_name_dict[n])\n","  \n","  for j in dict_band:\n","    if len(dict_band[j]) >= 2:\n","      for item in dict_band[j]:\n","        bucket[j].add(item)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-qE2PdhBM0-"},"source":["# Type your code here to do the fact check\n","# Calculate Jaccard similarity of the oiginal document with only \n","    # candidate documents\n","\n","original = 'Original_Sample/orig_taska.txt'\n","\n","# STEP-1: Generate 5-shingles \n","    # (if any shingles are no t present in your shingle index, simply ignore them)\n","\n","file = open(original, 'r',encoding='utf-8',errors='ignore')\n","\n","data = file.read()\n","\n","doc_shingles = []\n","l = data.lower().strip().split()\n","for i in range(len(l)-k+1):\n","  k_sh = tuple(l[i:i+k])\n","  doc_shingles.append(k_sh)\n","\n","doc_shingles = [' '.join(item) for item in doc_shingles]\n","    \n","# STEP-2: Generate signature vector from L hash functions\n","Mo = np.full((L,1), np.inf) \n","\n","for j in range(N):\n","  if shingles_set_list[j] in doc_shingles:\n","    for i in range(L):\n","      if hash_functions[i][j] < Mo[i,0]:\n","        Mo[i,0] = hash_functions[i][j] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8USRvTiWBM1A","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601605208465,"user_tz":300,"elapsed":360,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"61cb4e8d-ddc3-413a-f445-cbdc434091c5"},"source":["bucket_o = collections.defaultdict(set)\n","\n","Mb_o = np.reshape(Mo,[b,r,1]) \n","\n","# STEP - 2: Hash using the same hash functions created for \n","    # signature matrix hashing (in the previous cell)\n","\n","for i in range(b):\n","  dict_band = collections.defaultdict(list)\n","  \n","  x = Mb_o[i,:,0]\n","  h = (np.sum(ar*x)) % B\n","  bucket_o[h].add(i)\n","\n","dict_check = collections.defaultdict(set)\n","\n","for i in bucket_o:\n","  for item in bucket[i]:\n","    dict_check[0].add(item)\n","\n","lsh_list = []\n","file_name_dict_swap = {value:key for key, value in file_name_dict.items()}\n","\n","for item in dict_check[0]:\n","  lsh_list.append(file_name_dict_swap[item])\n","\n","print(f'Number of candidate documents is {len(lsh_list)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of candidate documents is 55\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y2LSvAri5jYs","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1601605209810,"user_tz":300,"elapsed":372,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"db2c28df-654a-426e-842c-3d0754286f54"},"source":["# STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.\n","    # and all other documents \n","M_lsh = M[:,lsh_list]\n","similar_docs_lsh = {}\n","\n","for i in range(M_lsh.shape[1]):\n","  same = M_lsh[:,i] == Mo[:,0]\n","  count = sum(same)/L\n","  if count >= t:\n","    similar_docs_lsh[file_name_dict[lsh_list[i]]] = count*100\n","    # print(i, ' ', lsh_list[i], ' ', file_name_dict[lsh_list[i]], ' ', count*100)\n","\n","similar_docs_lsh_sorted = {k: v for k, v in sorted(similar_docs_lsh.items(), key=lambda item: item[1])}\n","\n","print (\"{:<10} {:<25} {:<10} \".format('L', 'Name of the document', 'Jaccard similarity'))\n","for key,value in similar_docs_lsh_sorted.items():\n","  print (\"{:<10} {:<25} {:<10} \".format(L, key, round(value,2)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["L          Name of the document      Jaccard similarity \n","1000       g0pD_taska.txt            21.0       \n","1000       g3pC_taska.txt            33.7       \n","1000       g2pC_taska.txt            34.1       \n","1000       g4pC_taska.txt            84.2       \n","1000       g0pE_taska.txt            87.8       \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fS2NDq-jBM1C"},"source":["Report all documents (file_names) below that have Jaccard similarity > t=0.85\n","Sort the documents in decreasing order of the Jaccard similarity\n","\n","L          Name of the document      Jaccard similarity \n","\n","1000       g0pD_taska.txt            24.2       \n","1000       g3pC_taska.txt            32.7       \n","1000       g2pC_taska.txt            35.2       \n","1000       g4pC_taska.txt            83.9       \n","1000       g0pE_taska.txt            89.3 \n","\n"]},{"cell_type":"markdown","metadata":{"id":"DKymvddYBM1F"},"source":["Report the list of false positives and false negatives below\n","\n"]},{"cell_type":"code","metadata":{"id":"srfR4Xx0_1pi","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1601605213074,"user_tz":300,"elapsed":218,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"88213b3a-61d3-464a-9a04-0141dafea41c"},"source":["false_positives = len(lsh_list) - len(similar_docs_lsh_sorted)\n","false_negatives = len_sim_docs - len(similar_docs_lsh_sorted)\n","\n","print(f'Number of false positives = {false_positives}')\n","print(f'Number of false negatives = {false_negatives}')\n","\n","fp_list = []\n","fn_list = []\n","\n","similar_docs_list = [key for key,value in similar_docs_sorted.items()]\n","similar_docs_lsh_list = [key for key,value in similar_docs_lsh_sorted.items()]\n","\n","for key in dict_check[0]:\n","  if key not in similar_docs_list:\n","    fp_list.append(key)\n","\n","for key in similar_docs_list:\n","  if key not in dict_check[0]:\n","    fn_list.append(key)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of false positives = 50\n","Number of false negatives = 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cozFu7yQBM1F","colab":{"base_uri":"https://localhost:8080/","height":878},"executionInfo":{"status":"ok","timestamp":1601605214384,"user_tz":300,"elapsed":249,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"6476396a-686b-4b3f-a745-b4178a1a0297"},"source":["print('The list of false positives is as below : ')\n","for i in fp_list:\n","  print(i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The list of false positives is as below : \n","g3pB_taskb.txt\n","g3pA_taskc.txt\n","g0pE_taskc.txt\n","g4pD_taskb.txt\n","g2pE_taskb.txt\n","g2pE_taska.txt\n","g1pB_taskb.txt\n","g0pC_taskb.txt\n","g0pE_taskb.txt\n","g4pC_taskc.txt\n","g0pA_taskb.txt\n","g4pB_taska.txt\n","g4pD_taskc.txt\n","g3pB_taskc.txt\n","g3pC_taskb.txt\n","g4pD_taska.txt\n","g1pB_taskc.txt\n","g2pE_taskc.txt\n","g3pC_taskc.txt\n","g4pE_taskb.txt\n","g1pD_taska.txt\n","g2pB_taska.txt\n","g0pD_taskb.txt\n","g0pC_taskc.txt\n","g4pE_taskc.txt\n","g2pB_taskb.txt\n","g2pC_taskc.txt\n","g1pD_taskb.txt\n","g1pA_taskb.txt\n","g0pD_taskc.txt\n","g0pC_taska.txt\n","g1pB_taska.txt\n","g0pB_taskb.txt\n","g2pC_taskb.txt\n","g2pB_taskc.txt\n","g1pA_taskc.txt\n","g4pC_taskb.txt\n","g0pB_taskc.txt\n","g4pB_taskb.txt\n","g4pE_taska.txt\n","g0pA_taska.txt\n","g0pB_taska.txt\n","g2pA_taska.txt\n","g2pA_taskb.txt\n","g4pB_taskc.txt\n","g0pA_taskc.txt\n","g2pA_taskc.txt\n","g3pB_taska.txt\n","g1pA_taska.txt\n","g1pD_taskc.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zZQ64p_d_uuM","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601605215778,"user_tz":300,"elapsed":232,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"f070613e-0594-41f8-e87b-f9d3fa7f136a"},"source":["print('The list of false negatives is as below : ')\n","for i in fn_list:\n","  print(i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The list of false negatives is as below : \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2IEWC4HeBM1I"},"source":["### BONUS : 10 points"]},{"cell_type":"code","metadata":{"id":"XofoxE3pBM1J"},"source":["# Experiment with different values of b,r,B, and t\n","    # to reduce the number of false positives and false negatives\n","    # Report all results in a table in a separate word document"],"execution_count":null,"outputs":[]}]}