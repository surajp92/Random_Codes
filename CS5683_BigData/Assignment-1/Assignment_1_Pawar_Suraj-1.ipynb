{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment_1_Pawar_Suraj-1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"KTadYLtNjJ35"},"source":["# CS 5683 - Big Data Analytics\n","## Assignment - 1: Intro. to Spark and RDD"]},{"cell_type":"markdown","metadata":{"id":"bN7HOoEPjJ39"},"source":["###### Use Google Colab to use this notebook\n","###### Let's setup Spark first"]},{"cell_type":"code","metadata":{"id":"zLB5jvinjJ4A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607630405078,"user_tz":360,"elapsed":37963,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"b8edb41b-5206-43b9-a23b-e8819bf2c996"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 69kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 53.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=d6719781f128b1a24589ca17b0276e6fb5008652165f630fd028443f5b6d89e0\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P7tyjFqFjg_6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607630224163,"user_tz":360,"elapsed":17257,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"088c11ae-f093-490d-a7d7-3c14daff0dff"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Omp99JzCjJ4M"},"source":["###### Import required libraries now"]},{"cell_type":"code","metadata":{"id":"fuGK4w2MjJ4O"},"source":["import os\n","import sys\n","from pyspark import SparkContext, SparkConf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhMbfFXyjJ4e"},"source":["###### Let's initialize Spark context now"]},{"cell_type":"code","metadata":{"id":"3n7L3XX1jJ4g"},"source":["# create Spark context with necessary configuration\n","sc = SparkContext(\"local\",\"PySpark Word Count Exmaple\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GIu4scajJ4q"},"source":["###### Follow the tutorial to mount your Google Drive. Give mounted Drive paths below"]},{"cell_type":"code","metadata":{"id":"HOZS1EZyjJ4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607630435768,"user_tz":360,"elapsed":1265,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"d22656a3-46dd-46ff-c526-298ef6aaebeb"},"source":["# Give **.txt FILE PATHS** here\n","file1 = '/content/drive/My Drive/file1.txt' # change the path  \n","file2 = '/content/drive/My Drive/file2.txt' # change the path  \n","\n","filer = open('/content/drive/My Drive/file1.txt', 'r') \n","Lines = filer.readlines()\n","\n","count = 0\n","# Strips the newline character \n","for line in Lines: \n","    print(\"Line{}: {}\".format(count, line.strip())) \n","\n","\n","# USE THESE FILES as input(s) FOR ALL BELOW QUESTIONS"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Line0: my name is Suraj\n","Line0: my name is SuraJ\n","Line0: my name is Suraj\n","Line0: my name is Suraj1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cSe7Ow33jJ5D"},"source":["### Example Spark program"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3aWOy6VPiHI","executionInfo":{"status":"ok","timestamp":1607630860704,"user_tz":360,"elapsed":842,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"6ddea166-0ecb-4a04-bc30-f5b3168896d1"},"source":["fileRDD = sc.textFile(file1)\n","wordsRDD = fileRDD.flatMap(lambda line: line.strip().split(' '))\n","wordsRDD = wordsRDD.filter(lambda word: word.lower()[-1].isdigit() == True)\n","# pairRDD = wordsRDD.map(lambda word: (word,1))\n","# countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n","wordsRDD.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Suraj1']"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"zUFzUMU5jJ5F"},"source":["# Example Spark application for a simple wordcount\n","# What is wordcount? \n","    # Given a file, count the frequency of all words appearing in that file\n","    \n","# Step-1: Read the required file. In our case it is file1 or file2.\n","# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n","fileRDD = sc.textFile(file1)\n","\n","# Step-2: \n","    # Each line in our file(s) is a sentence. So, we need to split the sentence with ' ' to get words\n","    # Using map() will return RDD[list]. But we need RDD[string]. So we use flatMap()\n","wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n","\n","wordsRDDm = fileRDD.map(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n","\n","# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n","pairRDD = wordsRDDm.map(lambda word: (word,1))\n","\n","# Step-4: Now we have to sum all 1's of each word\n","# NOTE: A word may present in multiple data partitions. So we use reduceByKey() to group by key and perform sum\n","countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n","\n","#Step-5: Save results in a text file\n","#pairRDD.saveAsTextFile('/content/drive/My Drive/Coursework/CS 5683_Big_Data/file13.txt') # <----------- GIVE FILE PATH\n","countRDD.saveAsTextFile('/content/drive/My Drive/file10.txt') # <----------- GIVE FILE PATH"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOo64tTZjJ5T"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UNXCpUAjJ5Z"},"source":["### Question - 1 (10 points) "]},{"cell_type":"markdown","metadata":{"id":"WJu5rq5njJ5a"},"source":["Write a Spark application that counts number of words that ends with each letter in file1. That is for each letter count the number of (non-unique) words that ends with the specific letter. You can **ignore** letter cases (consider the given text contains only lower-case letters). Also, you can **ignore** words that end with non-alphabetic letters. **Sort the output in alphabetical order**\n","\n","> Indented block\n","\n","\n","\n","Example Output:\n","('a', 500)\n","('B', 100)\n","\n","which means that the given input has 500 words that end with the letter 'a' and 100 words that end with letter 'B'.\n","NOTE: The output is sorted, the application counts duplicate words also, and the application is not case-sensitive"]},{"cell_type":"code","metadata":{"id":"v2gCz6UPjJ5b"},"source":["# YOUR CODE for Question-1 HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cqyjNopjJ5h"},"source":["fileRDD = sc.textFile(file1)\n","\n","wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <-------------------- TEST what happens when you use map()\n","\n","wordsRDDf = wordsRDD.filter(lambda word: word.lower()[-1].isdigit() == False) # remove words that ends with digits\n","\n","pairRDD = wordsRDDf.map(lambda word: (word[-1],1)) # map with key = last letter of word, and value = 1\n","\n","countRDD = pairRDD.reduceByKey(lambda a,b: a+b) # reduce keys with summation operation\n","\n","countRDDsort = countRDD.sortByKey(ascending=True) # sort using keys in ascending order\n","\n","countRDDsort.saveAsTextFile('/content/drive/My Drive/file_sol_q1.txt') # <----------- GIVE FILE PATH\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WSH2TuJjJ5t","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"1b82baad-4f6f-478a-fe8b-551c42b5711c"},"source":["# PRINT THE OUTPUT HERE\n","for x in countRDDsort.collect():\n","  print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('J', 1)\n","('e', 4)\n","('j', 2)\n","('s', 4)\n","('y', 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nXB6v_FljJ51"},"source":["### Question - 2 (10 points)"]},{"cell_type":"markdown","metadata":{"id":"j1qZbbb5jJ53"},"source":["Write a Spark application that counts the **unique** number of words that have **n-letters** in file1. That is count the number of **unique** 1-letter words, 2-letter words, 3-letter words, etc. You can **include** all non-alphabetic letters for this application.\n","\n","Example Output:\n","(1, 100)\n","(2, 700)\n","(3, 1500)\n","\n","which means that the input file for the Spark application has **100 unique 1-letter words** , **700 unique 2-letter words** , and **1500 uniqe 3-letter words**"]},{"cell_type":"code","metadata":{"id":"6JftlwI5jJ55"},"source":["# YOUR CODE for Question-2 HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiD217uqjJ59"},"source":["fileRDD = sc.textFile(file1)\n","\n","wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n","\n","distinctwordsRDD = wordsRDD.distinct() # remove duplicate entries\n","\n","pairRDD = distinctwordsRDD.map(lambda word: (len(word),1)) # map with key = length of word, and value = 1\n","\n","countRDD = pairRDD.reduceByKey(lambda a,b: a+b) # reduce keys with summation operation\n","\n","countRDDsort = countRDD.sortByKey(ascending=True) # sort using keys in ascending order\n","\n","countRDDsort.saveAsTextFile('/content/drive/My Drive/file_sol_q2.txt') # <----------- GIVE FILE PATH"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENZmaA18jJ6E"},"source":["# PRINT THE OUTPUT HERE\n","for x in countRDDsort.collect():\n","  print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lgCUy52djJ6K"},"source":["### Question - 3 (10 points)"]},{"cell_type":"markdown","metadata":{"id":"w-gAQK--jJ6M"},"source":["Write a Spark aplication that outputs wordcount from two files: file1 and file2. That is count the number of occurances of words from two files. You can **ignore** letter cases (consider the given input files contain only lower-case letters). Also, you can **ignore** that is not present in both files.\n","\n","Example Output:\n","(big, (10, 20))\n","(Data, (30, 50))\n","\n","which means the word \"big\" appears 10 times in file-1 and 20 times in file-2 and the word \"Data\" appears 30 times in file-1 and 50 times in file-2"]},{"cell_type":"code","metadata":{"id":"7HCEWu_gjJ6N"},"source":["# YOUR CODE for Question-3 HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlbvSSjVjJ6W"},"source":["fileRDD1 = sc.textFile(file1)\n","fileRDD2 = sc.textFile(file2)\n","\n","wordsRDD1 = fileRDD1.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n","wordsRDD2 = fileRDD2.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n","\n","pairRDD1 = wordsRDD1.map(lambda word: (word,1)) # map with key = word, and value = 1\n","pairRDD2 = wordsRDD2.map(lambda word: (word,1)) # map with key = word, and value = 1\n","\n","countRDD1 = pairRDD1.reduceByKey(lambda a,b: a+b) # reduce keys with summation operation\n","countRDD2 = pairRDD2.reduceByKey(lambda a,b: a+b) # reduce keys with summation operation\n","\n","countRDD3 = countRDD1.join(countRDD2).reduceByKey(lambda x,y : (x,y)) # join operation\n","#countRDD3 = countRDD1.union(countRDD2).reduceByKey(lambda x,y : (x,y)) # union operation\n","\n","# countRDD3.saveAsTextFile('/content/drive/My Drive/file_sol_q3.txt') # <----------- GIVE FILE PATH"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQSkhDrEjJ6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607631219097,"user_tz":360,"elapsed":1719,"user":{"displayName":"Suraj Pawar","photoUrl":"","userId":"02399728726760386482"}},"outputId":"c2d56c35-ed47-425d-97e4-19a7cc196fb3"},"source":["# PRINT THE OUTPUT HERE\n","for x in countRDD3.collect():\n","  print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('my', (4, 2))\n","('name', (4, 2))\n","('is', (4, 2))\n","('Suraj', (2, 1))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7IBqnhU7jJ6m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UGDKrnFBjJ6r"},"source":["### WHAT TO TURN-IN IN CANVAS"]},{"cell_type":"markdown","metadata":{"id":"_l0s8bZKjJ6s"},"source":["1. Complete questions 1,2, and 3\n","2. Download this completed python notebook as .ipynb\n","3. Upload it in Canvas assignment"]},{"cell_type":"markdown","metadata":{"id":"o0tK_92CjJ6t"},"source":["# Due Date: Sept. 3 at 11:59pm"]},{"cell_type":"code","metadata":{"id":"fo0vguKEjJ6u"},"source":["rdd = sc.parallelize(data)\r\n","rdd = rdd.filter(lambda x: x[1] == 'Biology').map(lambda x: [x[2],(x[3],1)])\r\n","rdd = rdd.reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])).map(lambda x: [x[0],x[1][0]/x[1][1]])\r\n","rdd = rdd.sortBy(lambda a: a[1])"],"execution_count":null,"outputs":[]}]}